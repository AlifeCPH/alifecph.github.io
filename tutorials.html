<!DOCTYPE HTML>
<!--
	Escape Velocity by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ALIFE 2024 Copenhagen</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" type="text/css" href="css/main.css">
		<link rel="icon" type="image/x-icon" href="images/Icon_ALIFE2024.png">
	</head>
	<body class="no-sidebar is-preload">
		<div id="page-wrapper">

			<!-- Header -->			
			<section id="header" class="wrapper" style="background: url('images/chairs2small.jpg')">

				<!-- Logo -->
				<div id="logo">
					<img a href="index.html" , src="images/Logo_ALIFE2024_grayinv.png" alt="ALIFE 2024 logo" style="height: 9em;">
					<h1>Tutorials</h1>
				</div>

				<!-- Nav -->
				<header-nav-component></header-nav-component>
			</section>

			<!-- Venue -->
			<section id="main" class="wrapper style3">
				<div class="title">Tutorials</div>
				<div class="container">

					<!-- Image 
					<a href="#" class="image featured">
						<img src=imgs/toolazy.png alt="" />
					</a>-->

					<hr>

					
					<div id="content">
						<article class="box post">

							<div class="row aln-center">
								<div class="col-12 col-12-medium">


									<header class="style1" style="padding:1em"><h2 id="panel">Engineering the open-ended evolution of synthetic biology</a></h2></header>
									<details>
									
									<p>Advances in the field of synthetic biology have expanded our ability to engineer living systems – be they molecules, networks, genomes or ecosystems – with new-to-nature functionalities. Yet, how we design living systems seems at odds to how they have emerged via evolution and often ignores their inherent capability to further evolve. In this workshop, we will consider this issue and explore 'evotype engineering', where a bioengineer not only designs the function a biosystem must perform, but also sculpts its evolutionary capacity towards being more robust or adaptable when used. We will explore this idea from the perspective of open-ended search and how we might build strategies that enable continuous evolution and innovations that can underpin more powerful engineering biology workflows. We welcome contributions at the intersection of synthetic biology, evolution and open-endedness from experimental, computational and theoretical perspectives.</p>

									
									<h3>Organizers</h3>
									<ul>
										<li>Michiel Stock - KERMIT - Ghent University</li>
										<li>Thomas Gorochowski - Biocompute lab - Bristol University</li>
										<li>Simeon Castle - Biocompute lab - Bristol University</li>
									</ul>
									
									
									</details>
									<hr>

									<header class="style1" style="padding:1em"><h2 id="panel">Automatic Design of Robot Bodies and Brains with Evolutionary Algorithms</a></h2></header>
									<details>

									<p>The evolution of robot bodies and brains allows researchers to investigate which building blocks are interesting for evolving artificial life. Agnostic to the evolutionary approach used, the supplied building blocks influence how artificial organisms will behave. What should these building blocks look like? How should we associate control units with these building blocks? How should we represent the genomes of these robots?</p>

									<p>The tutorial will be divided into two main parts: Theory and practice.</p>

									<p>In the theory session, we will first introduce the most common ways to represent and evolve robot controllers and bodies. For control, we will discuss approaches based on parametrized, oscillating patterns and more complex methods based on evolving Artificial Neural Networks. For the robot body, we will focus on modular robots: How do we represent robot body modules so that we can efficiently evolve well-performing designs? Finally, we will touch upon interactions between robot body and brain: What opportunities and challenges arise when working with these two aspects together?</p>

									<p>In the practical part of the tutorial, we will look into frameworks supporting evolving bodies and brains. There are many existing frameworks and simulation approaches tailored to experimenting with body brain co-optimization and we have been using several ourselves. One goal in this tutorial is to give the audience a wide overview of existing frameworks supporting evolutionary robotics, along with pros and cons of each one, to allow them to make their own decisions when setting up experiments. In addition to a broad overview, we will also go in depth on at least one framework, giving the participants guided hands-on experience with evolving bodies and brains.</p>

									<h3>Organizers</h3>
									<ul>
										<li>Kyrre Glette (University of Oslo)</li>
										<li>Kai Olav Ellefsen (University of Oslo)</li>
									</ul>
									
									</details>

									<hr>
									
									<header class="style1" style="padding:1em"><h2 id="panel">JAX for Scaling Up Artificial Life</a></h2></header>
									<details>

									<p>This tutorial will focus on showing how to scale up and speed up ALife simulations through seamless hardware acceleration.</p>

									<p>First, we will give a crash course on JAX, a Python framework that allows for massively parallel computations using numpy-like syntax. We will show how to perform computations, automatic vectorize and parallelize to CPUs, GPUs and TPUs, and show how to render videos using 3D raycasting. Throughout this tutorial, we will use and share Google Colab notebooks. Colab is a fantastic way for sharing code and allowing for reproducibility of experiments. Afterwards, we will show two practical examples of ALife research where we used JAX.</p>

									<p>1) Lenia is a family of self-organizing morphogenetic systems. Several Lenia variants like Flow Lenia, Particle Lenia, and the recent Liquid Lenia have been implemented using JAX, with the benefits of easy scale-up, autograd, and smooth application of gradient descent and evolutionary algorithms. We will also take a glimpse of an upcoming JAX-based “Lenia Engine”.</p>

									<p>2) Biomaker CA is a framework that allows us to simulate Cellular Automata-based plant biomes. Plants are organisms composed by multiple CA cells, with each organism having their unique DNA. Plants reproduce with variation within the environment. Thanks to JAX parallelization, we can perform seamless meta-evolution such that our plants maximize a given goal. We will show parts of its implementation, how we render environments and how to perform different kinds of experiments. We will finally show how to implement new functionalities in Biomaker CA.</p>

									<h3>Organizers</h3>
									<ul>
										<li>Ettore Randazzo, Google Research, Zurich</li>
										<li>Bert Chan, Google DeepMind, Tokyo</li>
									</ul>
									
									</details>

									<hr>
									
									<header class="style1" style="padding:1em"><h2 id="panel">SimER: Simulation in Evolutionary Robotics</a></h2></header>
									<details>

									<p>ALIFE has a long history with virtual creatures, and the Virtual Creatures Competition has been a mainstay at the conference for a decade now. Enabling research in this area are many unique, and often single-purpose, simulation environments. The variety of simulation options is useful for creating finely tuned experiments but makes it difficult for newcomers to the field to determine which simulation environment is appropriate for their particular problem. ALIFE has also hosted several tutorials relating to this work (e.g., Avida-ED, neural networks and control, modular agent evolution, Unity-based evolutionary robotics). This tutorial, unlike others, will provide a broader look at the paradigms and tools used in the field of evolutionary robotics (and in ALIFE more generally). Specifically, we will discuss the use of several different simulation methods and tools, including: numerical simulation (e.g., Octave/Matlab, Python Sympy, Julia DynamicalSystems, etc.), physical simulation engines (e.g., ODE, DART, Bullet, etc.), soft body simulation (e.g., FEM, Voxelyze, etc.), comprehensive robot simulators (e.g., Gazebo, Webots, Isaac, etc.), and game engines (e.g., Unity, Unreal, Godot, etc.). We will also discuss visualization techniques. Of course, covering this wide range of topics necessarily means that we will not be able to go into great depth on all of them. Instead we will pay special attention to the use of physical simulation engines as they provide the widest range of possibilities with respect to creating and evolving virtual creatures. Staying with the theme of ALIFE 2024 we will share our experiences with "weird and wacky" results (and how to avoid them if they are not what you are looking for).</p>

									<h3>Organizers</h3>
									<ul>
										<li>Anthony J. Clark (Pomona College)</li>
										<li>Jared M. Moore (Grand Valley State University)</li>
									</ul>
									
									</details>

									<hr>
									
									<header class="style1" style="padding:1em"><h2 id="panel">Neuroevolution</a></h2></header>
									<details>

									<p>Neuroevolution, or optimization of neural networks through evolutionary computation, has been a growing subarea of machine learning & artificial life since the 1990s. Its primary focus has been on evolving neural networks for intelligent agents when the training targets are not known, and good performance requires many decisions over time, such as robotic control, game playing, and decision-making. More recently it has also been extended to optimizing deep-learning architectures, understanding how biological intelligence evolved, and optimizing neural networks for hardware implementation.</p>

									<p>This tutorial introduces students to the basics of neuroevolution, progresses to several advanced topics that make neuroevolution more effective and more general, reviews example application areas, and proposes further research questions. It builds on tutorials the organizers have given at conferences since 2005, including AAAI, GECCO, IJCNN, CEC, and Alife. They have been well-attended throughout, resulting in the establishment of a neuroevolution track at the GECCO conference starting in 2021. This tutorial is intended to provide a comprehensive methodological and practical foundation for researchers and practitioners to develop the technology further and to apply it to real-world problems.</p>
									
									<h3>Organizers</h3>
									<ul>
										<li>Sebastian Risi (IT University of Copenhagen)</li>
										<li>Risto Miikkulainen  (University of Texas at Austin)</li>
										<li>David Ha (Sakana AI)</li>
										<li>Yujin Tang (Sakana AI)</li>
									</ul>
									
									</details>

									<hr>
									
									<header class="style1" style="padding:1em"><h2 id="panel">Phylogenies: how and why to track them in artificial life</a></h2></header>
									<details>

									<p>Phylogenies (i.e., ancestry trees) group extant organisms by ancestral relatedness to render the history of hierarchical lineage branching events within an evolving system. These relationships reveal the evolutionary trajectories of  populations through a genotypic or phenotypic space. As such, phylogenies open a direct window through which to observe ecology, differential selection, genetic potentiation, emergence of complex traits, and other evolutionary dynamics in artificial life (ALife) systems. In evolutionary biology, phylogenies are often estimated from the fossil record, phenotypic traits, and extant genetic information. Although substantially limited in precision, such phylogenies have profoundly advanced our understanding of the evolution of life on Earth. In digital systems, we often have the ability to create perfect (or near perfect) phylogenies that reveal the step-by-step process by which evolution unfolds. However, phylogeny tracking and phylogeny-based analyses are not yet commonplace in ALife. Fortunately, a number of software tools have recently become available to facilitate such analyses, such as DEAP, Empirical, MABE, and hstrat.</p>

									<p>Biologists have developed many sophisticated and powerful phylogeny-based analysis techniques. For example, existing work uses properties of tree topology to infer characteristics of the evolutionary processes acting on a population. With an understanding of the differences between biology and artificial life, these approaches can be imported into ALife systems. For example, phylodiversity metrics can be used to detect diversity-maintaining ecological interactions and ongoing generation of significant evolutionary innovations.</p>
									
									<p>This tutorial will provide an introduction to phylogenies, how to record them in digital systems, and use cases for phylogenetic analyses in an artificial life context. We will open with a quick discussion of prior research enabled by and based on phylogenies in digital evolution systems. We will then survey existing phylogeny software tools and lead interactive tutorials on tracking phylogenies in both traditional and distributed computing environments. Next, we will discuss techniques for analyzing co-phylogenies (paired phylogenies of interacting species). Lastly, we will discuss open questions and future directions related to phylogenies in artificial life.</p>
									
									<h3>Organizers</h3>
									<ul>
										<li>Emily Dolson, Michigan State University</li>
										<li>Alexander Lalejini, Grand Valley State University</li>
										<li>Matthew Moreno, University of Michigan</li>
										<li>Jack Garbus, Brandeis University</li>
									</ul>
									
									</details>

									<hr>
									
									<header class="style1" style="padding:1em"><h2 id="panel">SwissGL/GPU: tiny libraries for tiny and beautiful programs on the web</a></h2></header>
									<details>

									<p>Artificial Life is beautiful to watch and fascinating to interact with, and the modern Web has graphics and compute capabilities to share this beauty with the world. Over the last few years I developed a number of GPU-powered Web-native interactive ALife simulations, including Neural Cellular Automata and various agent-based systems. My experience culminated in a few principles of building expressive minimalist GPU API abstraction layers, that facilitate development of interactive simulations and visualizations, and led to the development of SwissGL/GPU library.</p>

									<p>In this presentation I’m going to give a hands-on introduction into GPU programming for ALife by going through a number of examples built with a minimal set of tools and amount of code.</p>
									
									<h3>Organizers</h3>
									<ul>
										<li>Alexander Mordvinstsev, Google</li>
									</ul>
									
									</details>

									<hr>

								</div>
							</div>

						</article>
					</div>

					

				</div>
			</section>

			

			
			
			<!-- Footer -->
			<button onclick="topFunction()" id="myBtn" title="Go to top"><i id="iMyBtn" class="fa fa-chevron-up"></i></button>
			<footer-component></footer-component>

		</div>

		<!-- Scripts -->
<script src="js/jquery.min.js"></script>
<script src="js/jquery.dropotron.min.js"></script>
<script src="js/browser.min.js"></script>
<script src="js/breakpoints.min.js"></script>
<script src="js/util.js"></script>
<script src="js/main.js"></script>
<script src="js/headernav.js"></script>
<script src="js/footer.js"></script>

	</body>
</html>